---
date : '2025-08-05T14:44:25+08:00'
draft : false
title : '线性回归解析解的推导'
image: index.jpg
categories:
  - 学习
tags:
  - 算法
  - 机器学习
---
# 线性回归解析解的推导

---

## 问题形式化

我们要做的是：
首先给定一组数据
$$
(X,y)\quad其中X∈R^{n×d}，y∈R^n
$$

$$

$$


要拟合一个线性模型 
$$
y=Xw+b
$$
找到使预测和真实值之间**均方误差最小**的**w**和**b**。



其次，把每个样本的特征后面加上一列全是1的列，这样就可以把**b**作为**w**的一部分处理了。

所以，如果原始 **X** 是 n×d 矩阵，我们构造
$$
{X} = [X \quad \mathbf{1}]
$$


其中 **1**是 n×1 的全1列向量。
此时参数 
$$
~\tilde{w} 为(d+1) \times 1向量，最后一个元素就是b
$$

---

## 写出目标函数

线性回归的目标是最小化残差平方和（MSE）：
$$
L(\tilde{w}) = \| y - \tilde{X}\tilde{w} \|^2
$$
将其写成矩阵相乘的形式：
$$
L(w)=∥y−Xw∥^2=(y−Xw) ^⊤(y−Xw)
$$
展开目标函数为：
$$
L(w)=y^⊤y−2y^⊤Xw+w^⊤X^⊤Xw
$$
其中：
$$
𝑦^⊤𝑦y^⊤y 是常数项
$$

$$
−2𝑦^⊤𝑋𝑤−2y^⊤Xw 是一次项
$$

$$
w^⊤X^⊤Xw 是二次项
$$

---

## 解出解析解

现在来**对每一项分别求导**：
$$
y^\top y：跟w无关，导数是0
$$

$$
-2y^\top Xw：对w求导，就是-2X^\top y
$$

$$
w^\top X^\top Xw ：对w求导，就是 2X^\top X w
$$



------

这样，整个损失函数对w的导数是：
$$
\frac{\partial L}{\partial w} = 0 - 2X^\top y + 2X^\top X w\\= 2X^\top X w - 2X^\top y
$$
零其为0，得
$$
X^⊤Xw=X^⊤y
$$
两边左乘
$$
(𝑋^⊤𝑋)^{−1}
$$
得到
$$
w=(X^⊤X)^{−1}X^⊤y
$$

---

## 附

**矩阵相乘展开：**
$$
(a−b)^⊤(a−b)=a^⊤a−a^⊤b−b^⊤a+b^⊤b\\因为𝑎^⊤𝑏和𝑏^⊤𝑎是标量（它们互为转置，结果相同），所以上面中间两项可以合并为：\\𝑎⊤𝑎−2𝑎⊤𝑏+𝑏⊤𝑏
$$
**矩阵求导基础：**
$$
\frac{\partial}{\partial w}(w^\top A w) = 2Aw\quad（其中 A 是对称矩阵）
$$

$$
\frac{\partial}{\partial w}(b^\top w) = b\quad（b 是和 w 维度相同的向量）
$$

