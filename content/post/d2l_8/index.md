---
date : '2025-08-11T12:44:25+08:00'
draft : false
title : 'åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -4.6. æš‚é€€æ³•ï¼ˆDropoutï¼‰'
image: index.png
categories:
  - å­¦ä¹ 
tags:
  - å·¥ç¨‹å®è·µ
  - åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ 
---
# åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -4.6. æš‚é€€æ³•ï¼ˆDropoutï¼‰

---

## ä»£ç -ä»é›¶å¼€å§‹ç‰ˆæœ¬

```python
import torch
from torch import nn
from d2l import torch as d2l


def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¸¢å¼ƒ
    if dropout == 1:
        return torch.zeros_like(X)
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¿ç•™
    if dropout == 0:
        return X
    # dropoutè¡¨ç¤ºä¸¢å¼ƒæ¦‚ç‡ï¼Œæ¯”å¦‚è‹¥dropout=0.8ï¼Œéšæœºæ•°å°äº0.8çš„éƒ½ä¼šå˜æˆfalseï¼Œå³è¢«ä¸¢å¼ƒ
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)

X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
# è¿è¡Œç»“æœä¸ºï¼š
# tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
# tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
# tensor([[ 0.,  2.,  0.,  6.,  0.,  0.,  0.,  0.],
#         [ 0.,  0.,  0., 22.,  0., 26.,  0., 30.]])
# tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0., 0., 0., 0.]])

num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256

dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training = True):
        # ç»§æ‰¿è¶…ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # åªæœ‰åœ¨è®­ç»ƒæ¨¡å‹æ—¶æ‰ä½¿ç”¨dropout
        if self.training == True:
            # åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out

net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)

num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# ç´¯åŠ å™¨
class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]
# å‡†ç¡®ç‡
def accuracy(y_hat, y):
    # y_hat æ˜¯ logits æˆ– æ¦‚ç‡éƒ½å¯ä»¥ï¼›äºŒç»´æ—¶æŒ‰ç±»åˆ«ç»´å– argmax
    if y_hat.ndim > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(dim=1)
    return (y_hat.type(y.dtype) == y).float().mean().item()

# åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡
def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()
    metric = Accumulator(2)  # [é¢„æµ‹æ­£ç¡®æ•°, æ€»æ ·æœ¬æ•°]
    with torch.no_grad():
        for X, y in data_iter:
            metric.add((net(X).argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[1]

# è®­ç»ƒä¸€è½®
def train_epoch_ch3(net, train_iter, loss, updater):
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = Accumulator(3)  # [æŸå¤±å’Œ, é¢„æµ‹æ­£ç¡®æ•°, æ ·æœ¬æ€»æ•°]
    for X, y in train_iter:
        y_hat = net(X)
        l = loss(y_hat, y)            # è¿™é‡Œå…¼å®¹ CrossEntropyLoss(reduction='none')
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(X.shape[0])
        metric.add(l.sum(), (y_hat.argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]

# è®­ç»ƒä¸»æµç¨‹ï¼ˆå«ç®€å•æ‰“å°ï¼‰
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(f'epoch {epoch+1}: '
              f'loss {train_loss:.4f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')
train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# ç´¯åŠ å™¨
class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]
# å‡†ç¡®ç‡
def accuracy(y_hat, y):
    # y_hat æ˜¯ logits æˆ– æ¦‚ç‡éƒ½å¯ä»¥ï¼›äºŒç»´æ—¶æŒ‰ç±»åˆ«ç»´å– argmax
    if y_hat.ndim > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(dim=1)
    return (y_hat.type(y.dtype) == y).float().mean().item()

# åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡
def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()
    metric = Accumulator(2)  # [é¢„æµ‹æ­£ç¡®æ•°, æ€»æ ·æœ¬æ•°]
    with torch.no_grad():
        for X, y in data_iter:
            metric.add((net(X).argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[1]

# è®­ç»ƒä¸€è½®
def train_epoch_ch3(net, train_iter, loss, updater):
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = Accumulator(3)  # [æŸå¤±å’Œ, é¢„æµ‹æ­£ç¡®æ•°, æ ·æœ¬æ€»æ•°]
    for X, y in train_iter:
        y_hat = net(X)
        l = loss(y_hat, y)            # è¿™é‡Œå…¼å®¹ CrossEntropyLoss(reduction='none')
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(X.shape[0])
        metric.add(l.sum(), (y_hat.argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]

# è®­ç»ƒä¸»æµç¨‹ï¼ˆå«ç®€å•æ‰“å°ï¼‰
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(f'epoch {epoch+1}: '
              f'loss {train_loss:.4f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')
train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![result](result1.png)

## ä»£ç -ç®€æ´ç‰ˆæœ¬

```python
import torch
from torch import nn
from d2l import torch as d2l

dropout1, dropout2 = 0.2, 0.5
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
lr = 0.5
batch_size = 256
num_epochs=10
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
# ç´¯åŠ å™¨
class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n
    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
    def reset(self):
        self.data = [0.0] * len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]
# å‡†ç¡®ç‡
def accuracy(y_hat, y):
    # y_hat æ˜¯ logits æˆ– æ¦‚ç‡éƒ½å¯ä»¥ï¼›äºŒç»´æ—¶æŒ‰ç±»åˆ«ç»´å– argmax
    if y_hat.ndim > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(dim=1)
    return (y_hat.type(y.dtype) == y).float().mean().item()

# åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡
def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()
    metric = Accumulator(2)  # [é¢„æµ‹æ­£ç¡®æ•°, æ€»æ ·æœ¬æ•°]
    with torch.no_grad():
        for X, y in data_iter:
            metric.add((net(X).argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[1]

# è®­ç»ƒä¸€è½®
def train_epoch_ch3(net, train_iter, loss, updater):
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = Accumulator(3)  # [æŸå¤±å’Œ, é¢„æµ‹æ­£ç¡®æ•°, æ ·æœ¬æ€»æ•°]
    for X, y in train_iter:
        y_hat = net(X)
        l = loss(y_hat, y)            # è¿™é‡Œå…¼å®¹ CrossEntropyLoss(reduction='none')
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(X.shape[0])
        metric.add(l.sum(), (y_hat.argmax(dim=1) == y).sum(), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]

# è®­ç»ƒä¸»æµç¨‹ï¼ˆå«ç®€å•æ‰“å°ï¼‰
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(f'epoch {epoch+1}: '
              f'loss {train_loss:.4f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')

train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![result2](result2.png)

---

## é—®é¢˜æ€»ç»“

### é—®1ï¼šdropout_layer(X, dropout)å‡½æ•°ä¸­ä¸ºä»€ä¹ˆè¦é™¤ä»¥1-pï¼Ÿ

dropout_layer(X, dropout)æµç¨‹ï¼š

ï¼ˆ1ï¼‰æ£€æŸ¥å‚æ•°èŒƒå›´

```python
assert 0 <= dropout <= 1
```

ï¼ˆ2ï¼‰å¤„ç†æç«¯æƒ…å†µ

```python
if dropout == 1:
    return torch.zeros_like(X)  # å…¨ä¸¢å¼ƒ
if dropout == 0:
    return X                    # å…¨ä¿ç•™
```

ï¼ˆ3ï¼‰ç”Ÿæˆéšæœºæ©ç ï¼ˆmaskï¼‰

```python
mask = (torch.rand(X.shape) > dropout).float()
```

- torch.rand(X.shape)ç”Ÿæˆä¸Xå½¢çŠ¶ç›¸åŒçš„æ•°å€¼éšæœºåœ¨(0,1)çš„å‘é‡ï¼Œæ¯”å¦‚
- ä¸dropoutæ¯”è¾ƒå¾—åˆ°å¸ƒå°”çŸ©é˜µï¼ˆ`True` è¡¨ç¤ºä¿ç•™ï¼Œ`False` è¡¨ç¤ºä¸¢å¼ƒï¼‰
- .float()æŠŠå¸ƒå°”å€¼è½¬æˆ 1.0 / 0.0ï¼Œä¾¿äºæ•°å€¼è¿ç®—

æœ€ç»ˆå¾—åˆ°çš„maskå½¢å¼ä¸º

```python
tensor([[1., 0., 1., ...],
        [0., 1., 1., ...]])
```

ï¼ˆ4ï¼‰è¿”å›ç¼©æ”¾åçš„å‘é‡

```python
return mask * X / (1.0 - dropout)
```

#### ä¸ºä»€ä¹ˆè¦é™¤ä»¥1-pï¼Ÿ

ï¼ˆ1ï¼‰ä»ç›´è§‰ä¸Šç†è§£ï¼š

Dropout è®­ç»ƒæ—¶ä¼šéšæœºä¸¢ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼ˆè®¾ä¸¢å¼ƒæ¦‚ç‡ pï¼Œä¿ç•™æ¦‚ç‡ q=1âˆ’pï¼‰ã€‚

- å¦‚æœç›´æ¥ä¸¢æ‰ä¸€éƒ¨åˆ†ä¸åšè¡¥å¿ï¼Œè®­ç»ƒæ—¶ç¥ç»å…ƒçš„**å¹³å‡æ¿€æ´»å€¼ä¼šå˜å°**ï¼›
- ç„¶è€Œæ¨ç†æ—¶ä¸å¼€ Dropoutï¼ˆæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸ï¼‰ï¼Œå¹³å‡å€¼ä¼šå˜å¤§ â†’ åˆ†å¸ƒä¸ä¸€è‡´ â†’ å½±å“æ¨¡å‹ç¨³å®šæ€§ã€‚

**æ‰€ä»¥è¦åœ¨è®­ç»ƒæ—¶æ”¾å¤§ä¿ç•™çš„ç¥ç»å…ƒè¾“å‡ºï¼Œè®©æ•´ä½“å¹³å‡å€¼å’Œæ¨ç†æ—¶ä¸€è‡´ã€‚**

ï¼ˆ2ï¼‰ä»æ•°å­¦æœŸæœ›æ¥çœ‹

å‡è®¾æŸä¸ªç¥ç»å…ƒåœ¨ Dropout å‰çš„è¾“å‡ºæ˜¯ xï¼Œæ©ç å˜é‡ M æœä»äºŒé¡¹ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆM ä¹‹æ‰€ä»¥æœä» **ä¼¯åŠªåˆ©åˆ†å¸ƒ**ï¼Œæ˜¯å› ä¸º Dropout çš„æ ¸å¿ƒå°±æ˜¯**å¯¹æ¯ä¸ªç¥ç»å…ƒï¼ˆæˆ–ç‰¹å¾ï¼‰ç‹¬ç«‹åœ°åšä¸€æ¬¡äºŒé€‰ä¸€çš„éšæœºè¯•éªŒ**ï¼‰ï¼š
$$
Mâˆ¼Bernoulli(q)
\\å…¶ä¸­ï¼šğ‘ƒ(ğ‘€=1)=ğ‘=1âˆ’ğ‘ï¼ˆä¿ç•™æ¦‚ç‡ï¼‰\\ğ‘ƒ(ğ‘€=0)=ğ‘ï¼ˆä¸¢å¼ƒæ¦‚ç‡ï¼‰
$$
è‹¥æ²¡æœ‰é™¤ä»¥1-pï¼š
$$
è®­ç»ƒæ—¶è¾“å‡ºï¼šğ‘¦=ğ‘€â‹…ğ‘¥\\
æœŸæœ›ï¼šğ¸[ğ‘¦]=ğ¸[ğ‘€]â‹…ğ‘¥=ğ‘â‹…ğ‘¥\\
æ¨ç†æ—¶ï¼ˆä¸å¼€ Dropoutï¼‰è¾“å‡ºæ˜¯ ğ‘¥\\
æœŸæœ›ä¸ä¸€è‡´ï¼šğ¸_{train}[ğ‘¦]=ğ‘Â·ğ‘¥â‰ ğ‘¥=ğ¸_{test}[ğ‘¦]
$$
è‹¥é™¤ä»¥äº†1-pï¼š
$$
è®­ç»ƒæ—¶è¾“å‡ºï¼šğ‘¦=ğ‘€â‹…ğ‘¥/q\\
æœŸæœ›ï¼šğ¸[ğ‘¦]=ğ¸[ğ‘€]â‹…ğ‘¥/q=ğ‘¥\\
æœŸæœ›ä¸€è‡´ï¼šğ¸_{train}[ğ‘¦]=ğ¸_{test}[ğ‘¦]
$$
