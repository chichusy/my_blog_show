---
date : '2025-08-11T11:44:25+08:00'
draft : false
title : 'åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -4.5. æƒé‡è¡°å‡'
image: index.png
categories:
  - å­¦ä¹ 
tags:
  - å·¥ç¨‹å®è·µ
  - åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ 
---
# åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -4.5. æƒé‡è¡°å‡

---

## ä»£ç 

```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5
true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05
# d2l.synthetic_dataä¼šç”Ÿæˆç”Ÿæˆæ»¡è¶³ğ‘¦=ğ‘‹ğ‘¤+ğ‘+ğœ–çš„æ•°æ®ï¼Œè¿”å› (features, labels) ä¸¤ä¸ªå¼ é‡
train_data = d2l.synthetic_data(true_w, true_b, n_train)
# d2l.load_array((features, labels), batch_size, is_train)ç”¨ TensorDataset + DataLoader æ‰“åŒ…æˆå°æ‰¹é‡æ•°æ®è¿­ä»£å™¨ã€‚
# å¾—åˆ°çš„train_iter æ¯æ¬¡è¿­ä»£ç»™ä¸€æ‰¹ (X, y)ï¼Œå½¢çŠ¶åˆ†åˆ«æ˜¯ (batch_size, 200) å’Œ (batch_size, 1)
train_iter = d2l.load_array(train_data, batch_size)

test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

```python
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
```

```python
def train(lambd):
    w, b = init_params()
    # æ­¤å¤„çš„lossæ˜¯ä¸å¸¦L2æƒ©ç½šé¡¹çš„æŸå¤±å‡½æ•°
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            # å¢åŠ äº†L2èŒƒæ•°æƒ©ç½šé¡¹ï¼Œ
            # å¹¿æ’­æœºåˆ¶ä½¿l2_penalty(w)æˆä¸ºä¸€ä¸ªé•¿åº¦ä¸ºbatch_sizeçš„å‘é‡
            l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('wçš„L2èŒƒæ•°æ˜¯ï¼š', torch.norm(w).item())
```

```python
# ä¸ç”¨æ­£åˆ™åŒ–
train(lambd=0)
```

<img src="1.png" alt="1" style="zoom:67%;" />

```python
# é€‚ä¸­æ­£åˆ™åŒ–
train(lambd=3)
```

<img src="2.png" alt="2" style="zoom:67%;" />

```python
# å¼ºæ­£åˆ™åŒ–
train(lambd=10)
```

<img src="3.png" alt="3" style="zoom:67%;" />

```python
# ä½¿ç”¨æ¡†æ¶è‡ªå¸¦çš„æ–¹æ³•å®ç°L2æ­£åˆ™åŒ–
def train_concise(wd):
    net = nn.Sequential(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_()
    loss = nn.MSELoss(reduction='none')
    num_epochs, lr = 100, 0.003
    # net[0].weight â†’ è®¾ç½® weight_decay=wdï¼Œè¡¨ç¤ºåœ¨æ›´æ–°æ—¶è‡ªåŠ¨åŠ ä¸Š L2 æ­£åˆ™é¡¹æ¢¯åº¦
    # åç½®å‚æ•°æ²¡æœ‰è¡°å‡
    trainer = torch.optim.SGD([
        {"params":net[0].weight,'weight_decay': wd},
        {"params":net[0].bias}], lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad() # æ¸…é™¤ä¸Šä¸€æ­¥çš„æ¢¯åº¦
            # loss(...) è®¡ç®—é€æ ·æœ¬çš„ MSE æŸå¤±ï¼ˆä¸å«æ­£åˆ™é¡¹ï¼ŒL2 æ­£åˆ™åŒ–ç”±ä¼˜åŒ–å™¨åœ¨æ¢¯åº¦æ›´æ–°æ—¶è‡ªåŠ¨æ·»åŠ ï¼‰
            l = loss(net(X), y) # net(X)å‰å‘è®¡ç®—é¢„æµ‹å€¼
            # .mean()ï¼šè½¬æˆæ ‡é‡ï¼ˆæ‰¹å†…å¹³å‡ï¼‰ï¼Œè¿™æ · backward() æ‰èƒ½è¿è¡Œ 
            # .backward()ï¼šåå‘ä¼ æ’­ï¼Œè®¡ç®—çº¯æ•°æ®è¯¯å·®çš„æ¢¯åº¦ï¼›
            # ä¹‹ååœ¨ trainer.step() é˜¶æ®µï¼Œä¼˜åŒ–å™¨ä¼šåœ¨æ¢¯åº¦ä¸­é¢å¤–åŠ ä¸Š wd * wï¼ˆå®ç° L2 æ­£åˆ™åŒ–çš„æ•ˆæœï¼‰
            l.mean().backward()           
            trainer.step() # æŒ‰ SGD è§„åˆ™æ›´æ–°å‚æ•°
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('wçš„L2èŒƒæ•°ï¼š', net[0].weight.norm().item())
```

```python
train_concise(0)
```

<img src="4.png" alt="4" style="zoom:67%;" />

```python
train_concise(3)
```

<img src="5.png" alt="5" style="zoom:67%;" />

```python
train_concise(10)
```

<img src="6.png" alt="6" style="zoom:67%;" />

---

## é—®é¢˜æ€»ç»“

### é—®1ï¼šw.pow(2)å…·ä½“æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Œwä¸æ˜¯ä¸€ä¸ªçŸ©é˜µå—

`w` åœ¨è¿™é‡Œæ˜¯ä¸€ä¸ªå½¢çŠ¶ `(200, 1)` çš„**äºŒç»´å¼ é‡**ï¼ˆå¯ä»¥çœ‹ä½œ 200Ã—1 çŸ©é˜µï¼‰ï¼Œ`w.pow(2)` æ˜¯ **é€å…ƒç´ å¹³æ–¹** è¿ç®—ï¼Œä¸æ˜¯çŸ©é˜µä¹˜æ³•å¹³æ–¹ã€‚

å³å¯¹ `w` ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´  w_i_j å•ç‹¬å¹³æ–¹ï¼Œå¾—åˆ°çš„æ–°å¼ é‡çš„å½¢çŠ¶å’Œ `w` å®Œå…¨ä¸€æ ·ã€‚

### é—®2ï¼štrain()å‡½æ•°ä¸­lambdè¿™ä¸ªå‚æ•°æœ‰ä»€ä¹ˆæ„ä¹‰

`lambd` å°±æ˜¯ **L2 æ­£åˆ™åŒ–çš„ç³»æ•°**ï¼ˆä¹Ÿå«æƒé‡è¡°å‡ç³»æ•°ã€regularization coefficientï¼‰ï¼Œå®ƒç›´æ¥æ§åˆ¶äº†**æ­£åˆ™é¡¹åœ¨æ€»æŸå¤±ä¸­æ‰€å çš„æ¯”é‡**ã€‚

train()å‡½æ•°ä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°å½¢å¼ä¸ºï¼š

<img src="7.png" alt="7"  />
$$
Î» è¶Šå¤§ â†’ æ­£åˆ™åŒ–æƒ©ç½šé¡¹è¶Šé‡è¦ â†’ æ¨¡å‹ä¼šæ›´å¼ºçƒˆåœ°å‹ç¼©æƒé‡çš„å¤§å° â†’ æƒé‡çš„ L2 èŒƒæ•°âˆ¥ğ‘¤âˆ¥_2ä¼šå˜å°\\
Î» è¶Šå° â†’ æ­£åˆ™åŒ–çš„ä½œç”¨è¶Šå¼± â†’ æ›´æ¥è¿‘æ™®é€šçš„æœ€å°äºŒä¹˜å›å½’ã€‚
$$


### é—®3ï¼š **`w` çš„ L2 èŒƒæ•°**åœ¨è¿™é‡Œåˆ°åº•æ„å‘³ç€ä»€ä¹ˆ

 `w` æ˜¯ä¸€ä¸ª **(200Ã—1) çš„åˆ—å‘é‡**ï¼Œä»£è¡¨ 200 ä¸ªè¾“å…¥ç‰¹å¾çš„æƒé‡ç³»æ•°ã€‚

L2 èŒƒæ•°ï¼ˆEuclidean normï¼‰å°±æ˜¯æŠŠå®ƒçœ‹ä½œä¸€ä¸ª**ç‚¹**ï¼Œæµ‹é‡å®ƒç¦»åŸç‚¹æœ‰å¤šè¿œï¼š
$$
âˆ¥wâˆ¥_2 = \sqrt{w_1^2 +w_2^2 +â‹¯+w_{200}^2}
$$
è¿™å°±åƒæµ‹é‡ä¸€ä¸ª 200 ç»´ç©ºé—´é‡Œçš„å‘é‡çš„â€œ**é•¿åº¦**â€ï¼ˆå®ƒæ˜¯**æƒé‡å‘é‡çš„é•¿åº¦**ï¼Œåœ¨æ•°å­¦ä¸Šå°±æ˜¯åˆ°åŸç‚¹çš„è·ç¦»ï¼‰ã€‚

**å€¼è¶Šå¤§** â†’ æƒé‡æ•´ä½“å¹…åº¦è¶Šå¤§ï¼Œè¯´æ˜æ¨¡å‹æ›´â€œæ¿€è¿›â€ï¼Œå¯¹è¾“å…¥å˜åŒ–ååº”å¯èƒ½æ›´æ•æ„Ÿï¼Œè¿‡æ‹Ÿåˆé£é™©æ›´é«˜ã€‚

**å€¼è¶Šå°** â†’ æƒé‡æ•´ä½“å¹…åº¦è¶Šå°ï¼Œæ¨¡å‹æ›´å¹³æ»‘ï¼Œå¯¹æ–°æ•°æ®å¯èƒ½æ³›åŒ–æ›´å¥½ï¼ˆä½†ä¹Ÿå¯èƒ½æ¬ æ‹Ÿåˆï¼‰ã€‚

### 

### 
