<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="动手学深度学习-8.5. 循环神经网络的从零开始实现 代码 1 2 3 4 import collections import re from d2l import torch as d2l import random 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 # d2l.DATA_HUB是一个字典，用于存储数据集的下载地址和文件校验信息（哈希值） # key：数据集名称 &#39;time_machine&#39; # value：一个 tuple (url, sha1) d2l.DATA_HUB[&#39;time_machine&#39;] = (d2l.DATA_URL + &#39;timemachine.txt&#39;, &#39;090b5e7e70c295757f55df93cb0a180b9691891a&#39;) def read_time_machine(): #@save &#34;&#34;&#34;将时间机器数据集加载到文本行的列表中&#34;&#34;&#34; with open(d2l.download(&#39;time_machine&#39;), &#39;r&#39;) as f: lines = f.readlines() # &#39;[^A-Za-z]+&#39; 匹配 所有连续的非字母字符（空格、标点、数字等），并将其替换为空格 # .strip()删除字符串开头和结尾的空格 return [re.sub(&#39;[^A-Za-z]+&#39;, &#39; &#39;, line).strip().lower() for line in lines] # 把文本行拆分成词元（tokens），可以按单词或者按字符拆分 # lines为存储文本行的列表 def tokenize(lines, token=&#39;word&#39;): #@save &#34;&#34;&#34;将文本行拆分为单词或字符词元&#34;&#34;&#34; if token == &#39;word&#39;: # 注意line.split()本身是一个列表，这个分支情况的返回值形如：[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;], [&#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]] return [line.split() for line in lines] elif token == &#39;char&#39;: return [list(line) for line in lines] else: print(&#39;错误：未知词元类型：&#39; + token) def count_corpus(tokens): #@save &#34;&#34;&#34;统计词元的频率&#34;&#34;&#34; # 这里的tokens是1D列表或2D列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成一个列表（列表的嵌套表达式） # 外层：for line in tokens → 遍历 tokens 中的每一行（每一行本身是一个列表） # 内层：for token in line → 遍历这一行中的每个元素（词元） tokens = [token for line in tokens for token in line] # 返回的是一个字典，键是元素，值是出现次数。 return collections.Counter(tokens) # 词表类 class Vocab: &#34;&#34;&#34;文本词表&#34;&#34;&#34; def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] # reserved_tokens：保留的特殊词元列表 if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) # counter.items() 返回字典的 键值对列表，每个元素是 (key, value) 元组 # key=lambda x: x[1]表示按照value的值大小排序 self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # 生成一个列表，未知词元的索引为0 self.idx_to_token = [&#39;&lt;unk&gt;&#39;] + reserved_tokens # 生成一个字典，enumerate(self.idx_to_token) 会生成 (索引, 词元) 对 self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq &lt; min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) # 把一个词（或词列表）转换成它在词表中的索引 def __getitem__(self, tokens): # 如果tokens不是列表或元组 if not isinstance(tokens, (list, tuple)): # Python 字典 get 方法的标准用法，它接受 两个参数：dict.get(key, default) # 第一个参数 key：要查找的键，这里是 tokens（单个词）。 # 第二个参数 default：如果字典里没有这个键时返回的值，这里是 self.unk（未知词的索引，通常是 0）。 return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] # 把索引（或索引列表）转换回词 def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] # @property 是 Python 的一个 装饰器（decorator），用于把一个类的方法 变成属性（attribute） 来访问，而不需要加括号()调用 @property def unk(self): # 未知词元的索引为0 return 0 @property def token_freqs(self): return self._token_freqs # 返回时光机器数据集的词元索引列表和词表 def load_corpus_time_machine(max_tokens=-1): lines = read_time_machine() # 将每行文本拆成字符列表（因为 &#39;char&#39;），tokens 是一个二维列表 tokens = tokenize(lines, &#39;char&#39;) vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落， # 所以将所有文本行展平到一个列表中 # 当写 vocab[token] 时，Python 会自动调用 vocab.__getitem__(token)，返回对应的索引 # corpus是一个列表，按照原本文本的字符顺序，把各个字符在词表中对应的索引保存起来 corpus = [vocab[token] for line in tokens for token in line] if max_tokens &gt; 0: corpus = corpus[:max_tokens] return corpus, vocab # 使用随机抽样生成一个小批量子序列 # batch_size：每个小批量包含多少条子序列 # num_steps: 每个子序列的长度（即时间步数） def seq_data_iter_random(corpus, batch_size, num_steps): # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # num_subseqs表示可以切出多少个长度为 num_steps 的子序列 # 减去1，因为要给标签留一个位置 num_subseqs = (len(corpus) - 1) // num_steps # 构造所有子序列的起始索引，每隔 num_steps 取一个 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从pos位置开始的长度为num_steps的序列 return corpus[pos: pos + num_steps] # num_batches表示能完整分多少个 batch num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # 在这里，initial_indices包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) # 举例：假设corpus = list(range(30)),就是[0,1,2,...,29],batch_size = 2,num_steps = 5 # num_subseqs = (30-1)//5 = 5,所以一共可以切 5 段子序列，每段长 5： # [0,1,2,3,4] # [5,6,7,8,9] # [10,11,12,13,14] # [15,16,17,18,19] # [20,21,22,23,24] # 得到 initial_indices = [0, 5, 10, 15, 20] ,打乱后：[15, 0, 20, 10, 5] # num_batches = 5 // 2 = 2，所以总共能生成 2 个 batch。 # 循环展开： # i=0时：initial_indices_per_batch = [15, 0]，X = [data(15), data(0)]，Y = [data(16), data(1)] # 得到X = [[15,16,17,18,19],[0,1,2,3,4]]，Y = [[16,17,18,19,20],[1,2,3,4,5]] # i=2时：initial_indices_per_batch = [20, 10]，X = [data(20), data(10)]，Y = [data(21), data(11)] # 得到X = [[20,21,22,23,24],[10,11,12,13,14]]，Y = [[21,22,23,24,25],[11,12,13,14,15]] # 使用顺序分区生成一个小批量子序列 def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y class SeqDataLoader: #@save &#34;&#34;&#34;加载序列数据的迭代器&#34;&#34;&#34; def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = seq_data_iter_random else: self.data_iter_fn = seq_data_iter_sequential self.corpus, self.vocab = load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps # 当写 for X, Y in train_iter: 时，Python 会自动调用 train_iter.__iter__() def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps) def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000): &#34;&#34;&#34;返回时光机器数据集的迭代器和词表&#34;&#34;&#34; data_iter = SeqDataLoader( batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab 1 2 3 4 5 %matplotlib inline import math import torch from torch import nn from torch.nn import functional as F 1 2 batch_size, num_steps = 32, 35 train_iter, vocab = load_data_time_machine(batch_size, num_steps) 1 F.one_hot(torch.tensor([0, 2]), len(vocab)) 1 2 3 4 5 6 7 8 9 # 利用jupyter notebook测试上述代码 vocab[&#39;a&#39;] # 返回4 vocab.token_to_idx[&#39;a&#39;]==vocab[&#39;a&#39;] # 返回True vocab.idx_to_token[0] # 返回&#39;&lt;unk&gt;&#39; vocab.idx_to_token[2] # 返回&#39;e&#39; # 返回torch.Size([5, 2, 28]) X = torch.arange(10).reshape((2, 5)) F.one_hot(X.T, 28).shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def get_params(vocab_size, num_hiddens, device): # 词表长度为28，因为输入时需要将每一个token进行独热编码，则此处输入长度应该也为28 num_inputs = num_outputs = vocab_size def normal(shape): # randn 的意思就是 random normal，它会从标准正态分布中采样数据，乘0.01后服从标准差为0.01的高斯分布 return torch.randn(size=shape, device=device) * 0.01 # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # 隐藏层参数 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # O_t = H_t·W_hq + b_q # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params 1 2 3 #返回隐藏层初始状态，此处为全零 def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # inputs形状为 (num_steps, batch_size, vocab_size) # state形状为 (batch_size, num_hiddens) def rnn(inputs, state, params): # inputs的形状：(时间步数量，批量大小，词表大小) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # X的形状：(batch_size, vocab_size) for X in inputs: # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # (batch_size, num_hiddens) = (batch_size, vocab_size)*(vocab_size, num_hiddens) + (batch_size, num_hiddens)*(num_hiddens, num_hiddens) + (num_hiddens,) # 其中b_h形状(num_hiddens,)会被解释成(1, num_hiddens) → 自动广播成 (batch_size, num_hiddens) # 在默认的一维 bias 情况下，PyTorch 总是在最前面加一个维度 → (1, num_hiddens),而不会被解释成(num_hiddens,1) H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) # O_t = H_t·W_hq + b_q # (batch_size, vocab_size) = (batch_size, num_hiddens)*(num_hiddens, vocab_size) + (vocab_size,) # # 其中b_q形状(vocab_size,)会被解释成(1, vocab_size) → 自动广播成 (batch_size, vocab_size) Y = torch.mm(H, W_hq) + b_q outputs.append(Y) # 沿着第一个维度将outputs数组进行拼接 # outputs本身为长度为num_steps的列表，outputs[i] 的形状就是 (batch_size, vocab_size) # 拼接后形状为(num_steps*batch_size, vocab_size) return torch.cat(outputs, dim=0), (H,) 1 2 3 4 5 6 7 8 9 10 11 12 13 class RNNModelScratch: &#34;&#34;&#34;从零开始实现的循环神经网络模型&#34;&#34;&#34; def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn # __call__ 函数在用类实例像函数一样调用时会自动执行 def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) 1 num_hiddens = 512 1 2 3 4 5 6 7 8 # net是类的实例对象 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) # 实例对象被像函数一样调用时自动触发call函数，net(...)等价于net.__call__(...) Y, new_state = net(X.to(d2l.try_gpu()), state) Y.shape, len(new_state), new_state[0].shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # prefix 是函数的一个输入参数，表示生成文本的前缀字符串 def predict_ch8(prefix, num_preds, net, vocab, device): &#34;&#34;&#34;在prefix后面生成新字符&#34;&#34;&#34; state = net.begin_state(batch_size=1, device=device) # 把 prefix 的第一个字符转换成对应的索引（通过词表 vocab）加入 outputs 列表 outputs = [vocab[prefix[0]]] # 定义一个匿名函数，每次调用返回当前 RNN 的输入，outputs[-1] 是上一个生成的字符的索引 get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) # 预热期，让 RNN 根据前缀 prefix “热身”，更新隐藏状态 for y in prefix[1:]: # _ 表示输出暂时不用，只更新隐藏状态，即state 会被更新，用于下一个时间步 _, state = net(get_input(), state) # 同时把前缀中对应字符的索引追加到 outputs列表中 outputs.append(vocab[y]) # 预测num_preds步 for _ in range(num_preds): y, state = net(get_input(), state) print(&#34;y的形状：&#34;,y.shape) print(&#34;y中得分最高的索引：&#34;,y.argmax(dim=1)) print(&#34;当前预测字符：&#34;,vocab.idx_to_token[int(y.argmax(dim=1))]) # 把预测出来的对应字符的索引追加到 outputs列表中 # y.argmax(dim=1) 取得分最高的索引 → 当前预测字符 outputs.append(int(y.argmax(dim=1).reshape(1))) print(&#34;outputs列表:&#34;,outputs) print(&#34;\\n&#34;) return &#39;&#39;.join([vocab.idx_to_token[i] for i in outputs]) 1 predict_ch8(&#39;time traveller &#39;, 10, net, vocab, d2l.try_gpu()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 梯度裁剪，防止梯度爆炸 def grad_clipping(net, theta): # 如果 net 是 PyTorch 模块，直接用 net.parameters() 获取可训练参数 if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params # 计算梯度范数（对所有参数的梯度求平方和，然后开平方） # 这一行的norm是一个标量，是params里所有参数（每一个参数是一个矩阵）的平方和在开方 norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) if norm &gt; theta: for param in params: # 修改过后的梯度向量的L2范数为theta param.grad[:] *= theta / norm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # 训练一个 epoch def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): state, timer = None, d2l.Timer() # 总损失 metric[0]，总词元数 metric[1] metric = d2l.Accumulator(2) # X形状为(batch_size, num_steps)，Y与X形状相同 for X, Y in train_iter: # 第一次迭代，或者使用随机采样时，每个小批量都重新初始化隐藏状态 if state is None or use_random_iter: state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state对于nn.GRU是个张量 state.detach_() else: # state对于nn.LSTM或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() # 先将Y的形状转置为(num_steps, batch_size),再按行展平成一维向量，形状为(batch_size * num_steps,) # 为什么要先转置？ 因为希望按照时间步顺序展平而不是批量顺序 # 展平后为（第1个时间步每个batch的第一个元素，然后第2个时间步每个batch的第二个元素，...） # 因为后续y_hat形状为(batch_size∗num_steps,vocab_size) # PyTorch 的 CrossEntropyLoss 要求： # input：形状 (N, C)，N 是样本数，C 是类别数。 # target：形状 (N,)，一维整数向量表示每个样本的类别。 y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1) updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了mean函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 训练整个RNN模型 def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;perplexity&#39;, legend=[&#39;train&#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(&#39;time traveller&#39;)) animator.add(epoch + 1, [ppl]) print(f&#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}&#39;) print(predict(&#39;time traveller&#39;)) print(predict(&#39;traveller&#39;)) 1 2 3 # 顺序分区（sequential） 生成训练小批量 num_epochs, lr = 500, 1 train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu()) 1 2 3 4 5 # 随机抽样（random）生成训练小批量 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True) ">
<title>动手学深度学习-8.5. 循环神经网络的从零开始实现</title>

<link rel='canonical' href='https://example.com/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/'>

<link rel="stylesheet" href="/scss/style.min.76d56cb8dfd27db635f813412aada78a21b66f7863415f1fa2503126dbbf502d.css"><meta property='og:title' content="动手学深度学习-8.5. 循环神经网络的从零开始实现">
<meta property='og:description' content="动手学深度学习-8.5. 循环神经网络的从零开始实现 代码 1 2 3 4 import collections import re from d2l import torch as d2l import random 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 # d2l.DATA_HUB是一个字典，用于存储数据集的下载地址和文件校验信息（哈希值） # key：数据集名称 &#39;time_machine&#39; # value：一个 tuple (url, sha1) d2l.DATA_HUB[&#39;time_machine&#39;] = (d2l.DATA_URL + &#39;timemachine.txt&#39;, &#39;090b5e7e70c295757f55df93cb0a180b9691891a&#39;) def read_time_machine(): #@save &#34;&#34;&#34;将时间机器数据集加载到文本行的列表中&#34;&#34;&#34; with open(d2l.download(&#39;time_machine&#39;), &#39;r&#39;) as f: lines = f.readlines() # &#39;[^A-Za-z]+&#39; 匹配 所有连续的非字母字符（空格、标点、数字等），并将其替换为空格 # .strip()删除字符串开头和结尾的空格 return [re.sub(&#39;[^A-Za-z]+&#39;, &#39; &#39;, line).strip().lower() for line in lines] # 把文本行拆分成词元（tokens），可以按单词或者按字符拆分 # lines为存储文本行的列表 def tokenize(lines, token=&#39;word&#39;): #@save &#34;&#34;&#34;将文本行拆分为单词或字符词元&#34;&#34;&#34; if token == &#39;word&#39;: # 注意line.split()本身是一个列表，这个分支情况的返回值形如：[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;], [&#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]] return [line.split() for line in lines] elif token == &#39;char&#39;: return [list(line) for line in lines] else: print(&#39;错误：未知词元类型：&#39; + token) def count_corpus(tokens): #@save &#34;&#34;&#34;统计词元的频率&#34;&#34;&#34; # 这里的tokens是1D列表或2D列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成一个列表（列表的嵌套表达式） # 外层：for line in tokens → 遍历 tokens 中的每一行（每一行本身是一个列表） # 内层：for token in line → 遍历这一行中的每个元素（词元） tokens = [token for line in tokens for token in line] # 返回的是一个字典，键是元素，值是出现次数。 return collections.Counter(tokens) # 词表类 class Vocab: &#34;&#34;&#34;文本词表&#34;&#34;&#34; def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] # reserved_tokens：保留的特殊词元列表 if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) # counter.items() 返回字典的 键值对列表，每个元素是 (key, value) 元组 # key=lambda x: x[1]表示按照value的值大小排序 self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # 生成一个列表，未知词元的索引为0 self.idx_to_token = [&#39;&lt;unk&gt;&#39;] + reserved_tokens # 生成一个字典，enumerate(self.idx_to_token) 会生成 (索引, 词元) 对 self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq &lt; min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) # 把一个词（或词列表）转换成它在词表中的索引 def __getitem__(self, tokens): # 如果tokens不是列表或元组 if not isinstance(tokens, (list, tuple)): # Python 字典 get 方法的标准用法，它接受 两个参数：dict.get(key, default) # 第一个参数 key：要查找的键，这里是 tokens（单个词）。 # 第二个参数 default：如果字典里没有这个键时返回的值，这里是 self.unk（未知词的索引，通常是 0）。 return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] # 把索引（或索引列表）转换回词 def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] # @property 是 Python 的一个 装饰器（decorator），用于把一个类的方法 变成属性（attribute） 来访问，而不需要加括号()调用 @property def unk(self): # 未知词元的索引为0 return 0 @property def token_freqs(self): return self._token_freqs # 返回时光机器数据集的词元索引列表和词表 def load_corpus_time_machine(max_tokens=-1): lines = read_time_machine() # 将每行文本拆成字符列表（因为 &#39;char&#39;），tokens 是一个二维列表 tokens = tokenize(lines, &#39;char&#39;) vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落， # 所以将所有文本行展平到一个列表中 # 当写 vocab[token] 时，Python 会自动调用 vocab.__getitem__(token)，返回对应的索引 # corpus是一个列表，按照原本文本的字符顺序，把各个字符在词表中对应的索引保存起来 corpus = [vocab[token] for line in tokens for token in line] if max_tokens &gt; 0: corpus = corpus[:max_tokens] return corpus, vocab # 使用随机抽样生成一个小批量子序列 # batch_size：每个小批量包含多少条子序列 # num_steps: 每个子序列的长度（即时间步数） def seq_data_iter_random(corpus, batch_size, num_steps): # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # num_subseqs表示可以切出多少个长度为 num_steps 的子序列 # 减去1，因为要给标签留一个位置 num_subseqs = (len(corpus) - 1) // num_steps # 构造所有子序列的起始索引，每隔 num_steps 取一个 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从pos位置开始的长度为num_steps的序列 return corpus[pos: pos + num_steps] # num_batches表示能完整分多少个 batch num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # 在这里，initial_indices包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) # 举例：假设corpus = list(range(30)),就是[0,1,2,...,29],batch_size = 2,num_steps = 5 # num_subseqs = (30-1)//5 = 5,所以一共可以切 5 段子序列，每段长 5： # [0,1,2,3,4] # [5,6,7,8,9] # [10,11,12,13,14] # [15,16,17,18,19] # [20,21,22,23,24] # 得到 initial_indices = [0, 5, 10, 15, 20] ,打乱后：[15, 0, 20, 10, 5] # num_batches = 5 // 2 = 2，所以总共能生成 2 个 batch。 # 循环展开： # i=0时：initial_indices_per_batch = [15, 0]，X = [data(15), data(0)]，Y = [data(16), data(1)] # 得到X = [[15,16,17,18,19],[0,1,2,3,4]]，Y = [[16,17,18,19,20],[1,2,3,4,5]] # i=2时：initial_indices_per_batch = [20, 10]，X = [data(20), data(10)]，Y = [data(21), data(11)] # 得到X = [[20,21,22,23,24],[10,11,12,13,14]]，Y = [[21,22,23,24,25],[11,12,13,14,15]] # 使用顺序分区生成一个小批量子序列 def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y class SeqDataLoader: #@save &#34;&#34;&#34;加载序列数据的迭代器&#34;&#34;&#34; def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = seq_data_iter_random else: self.data_iter_fn = seq_data_iter_sequential self.corpus, self.vocab = load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps # 当写 for X, Y in train_iter: 时，Python 会自动调用 train_iter.__iter__() def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps) def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000): &#34;&#34;&#34;返回时光机器数据集的迭代器和词表&#34;&#34;&#34; data_iter = SeqDataLoader( batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab 1 2 3 4 5 %matplotlib inline import math import torch from torch import nn from torch.nn import functional as F 1 2 batch_size, num_steps = 32, 35 train_iter, vocab = load_data_time_machine(batch_size, num_steps) 1 F.one_hot(torch.tensor([0, 2]), len(vocab)) 1 2 3 4 5 6 7 8 9 # 利用jupyter notebook测试上述代码 vocab[&#39;a&#39;] # 返回4 vocab.token_to_idx[&#39;a&#39;]==vocab[&#39;a&#39;] # 返回True vocab.idx_to_token[0] # 返回&#39;&lt;unk&gt;&#39; vocab.idx_to_token[2] # 返回&#39;e&#39; # 返回torch.Size([5, 2, 28]) X = torch.arange(10).reshape((2, 5)) F.one_hot(X.T, 28).shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def get_params(vocab_size, num_hiddens, device): # 词表长度为28，因为输入时需要将每一个token进行独热编码，则此处输入长度应该也为28 num_inputs = num_outputs = vocab_size def normal(shape): # randn 的意思就是 random normal，它会从标准正态分布中采样数据，乘0.01后服从标准差为0.01的高斯分布 return torch.randn(size=shape, device=device) * 0.01 # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # 隐藏层参数 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # O_t = H_t·W_hq + b_q # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params 1 2 3 #返回隐藏层初始状态，此处为全零 def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # inputs形状为 (num_steps, batch_size, vocab_size) # state形状为 (batch_size, num_hiddens) def rnn(inputs, state, params): # inputs的形状：(时间步数量，批量大小，词表大小) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # X的形状：(batch_size, vocab_size) for X in inputs: # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # (batch_size, num_hiddens) = (batch_size, vocab_size)*(vocab_size, num_hiddens) + (batch_size, num_hiddens)*(num_hiddens, num_hiddens) + (num_hiddens,) # 其中b_h形状(num_hiddens,)会被解释成(1, num_hiddens) → 自动广播成 (batch_size, num_hiddens) # 在默认的一维 bias 情况下，PyTorch 总是在最前面加一个维度 → (1, num_hiddens),而不会被解释成(num_hiddens,1) H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) # O_t = H_t·W_hq + b_q # (batch_size, vocab_size) = (batch_size, num_hiddens)*(num_hiddens, vocab_size) + (vocab_size,) # # 其中b_q形状(vocab_size,)会被解释成(1, vocab_size) → 自动广播成 (batch_size, vocab_size) Y = torch.mm(H, W_hq) + b_q outputs.append(Y) # 沿着第一个维度将outputs数组进行拼接 # outputs本身为长度为num_steps的列表，outputs[i] 的形状就是 (batch_size, vocab_size) # 拼接后形状为(num_steps*batch_size, vocab_size) return torch.cat(outputs, dim=0), (H,) 1 2 3 4 5 6 7 8 9 10 11 12 13 class RNNModelScratch: &#34;&#34;&#34;从零开始实现的循环神经网络模型&#34;&#34;&#34; def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn # __call__ 函数在用类实例像函数一样调用时会自动执行 def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) 1 num_hiddens = 512 1 2 3 4 5 6 7 8 # net是类的实例对象 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) # 实例对象被像函数一样调用时自动触发call函数，net(...)等价于net.__call__(...) Y, new_state = net(X.to(d2l.try_gpu()), state) Y.shape, len(new_state), new_state[0].shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # prefix 是函数的一个输入参数，表示生成文本的前缀字符串 def predict_ch8(prefix, num_preds, net, vocab, device): &#34;&#34;&#34;在prefix后面生成新字符&#34;&#34;&#34; state = net.begin_state(batch_size=1, device=device) # 把 prefix 的第一个字符转换成对应的索引（通过词表 vocab）加入 outputs 列表 outputs = [vocab[prefix[0]]] # 定义一个匿名函数，每次调用返回当前 RNN 的输入，outputs[-1] 是上一个生成的字符的索引 get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) # 预热期，让 RNN 根据前缀 prefix “热身”，更新隐藏状态 for y in prefix[1:]: # _ 表示输出暂时不用，只更新隐藏状态，即state 会被更新，用于下一个时间步 _, state = net(get_input(), state) # 同时把前缀中对应字符的索引追加到 outputs列表中 outputs.append(vocab[y]) # 预测num_preds步 for _ in range(num_preds): y, state = net(get_input(), state) print(&#34;y的形状：&#34;,y.shape) print(&#34;y中得分最高的索引：&#34;,y.argmax(dim=1)) print(&#34;当前预测字符：&#34;,vocab.idx_to_token[int(y.argmax(dim=1))]) # 把预测出来的对应字符的索引追加到 outputs列表中 # y.argmax(dim=1) 取得分最高的索引 → 当前预测字符 outputs.append(int(y.argmax(dim=1).reshape(1))) print(&#34;outputs列表:&#34;,outputs) print(&#34;\\n&#34;) return &#39;&#39;.join([vocab.idx_to_token[i] for i in outputs]) 1 predict_ch8(&#39;time traveller &#39;, 10, net, vocab, d2l.try_gpu()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 梯度裁剪，防止梯度爆炸 def grad_clipping(net, theta): # 如果 net 是 PyTorch 模块，直接用 net.parameters() 获取可训练参数 if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params # 计算梯度范数（对所有参数的梯度求平方和，然后开平方） # 这一行的norm是一个标量，是params里所有参数（每一个参数是一个矩阵）的平方和在开方 norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) if norm &gt; theta: for param in params: # 修改过后的梯度向量的L2范数为theta param.grad[:] *= theta / norm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # 训练一个 epoch def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): state, timer = None, d2l.Timer() # 总损失 metric[0]，总词元数 metric[1] metric = d2l.Accumulator(2) # X形状为(batch_size, num_steps)，Y与X形状相同 for X, Y in train_iter: # 第一次迭代，或者使用随机采样时，每个小批量都重新初始化隐藏状态 if state is None or use_random_iter: state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state对于nn.GRU是个张量 state.detach_() else: # state对于nn.LSTM或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() # 先将Y的形状转置为(num_steps, batch_size),再按行展平成一维向量，形状为(batch_size * num_steps,) # 为什么要先转置？ 因为希望按照时间步顺序展平而不是批量顺序 # 展平后为（第1个时间步每个batch的第一个元素，然后第2个时间步每个batch的第二个元素，...） # 因为后续y_hat形状为(batch_size∗num_steps,vocab_size) # PyTorch 的 CrossEntropyLoss 要求： # input：形状 (N, C)，N 是样本数，C 是类别数。 # target：形状 (N,)，一维整数向量表示每个样本的类别。 y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1) updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了mean函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 训练整个RNN模型 def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;perplexity&#39;, legend=[&#39;train&#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(&#39;time traveller&#39;)) animator.add(epoch + 1, [ppl]) print(f&#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}&#39;) print(predict(&#39;time traveller&#39;)) print(predict(&#39;traveller&#39;)) 1 2 3 # 顺序分区（sequential） 生成训练小批量 num_epochs, lr = 500, 1 train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu()) 1 2 3 4 5 # 随机抽样（random）生成训练小批量 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True) ">
<meta property='og:url' content='https://example.com/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/'>
<meta property='og:site_name' content='王涑毓的个人博客'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='工程实践' /><meta property='article:tag' content='动手学深度学习' /><meta property='article:published_time' content='2025-08-19T10:44:25&#43;08:00'/><meta property='article:modified_time' content='2025-08-19T10:44:25&#43;08:00'/><meta property='og:image' content='https://example.com/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/index.png' />
<meta name="twitter:title" content="动手学深度学习-8.5. 循环神经网络的从零开始实现">
<meta name="twitter:description" content="动手学深度学习-8.5. 循环神经网络的从零开始实现 代码 1 2 3 4 import collections import re from d2l import torch as d2l import random 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 # d2l.DATA_HUB是一个字典，用于存储数据集的下载地址和文件校验信息（哈希值） # key：数据集名称 &#39;time_machine&#39; # value：一个 tuple (url, sha1) d2l.DATA_HUB[&#39;time_machine&#39;] = (d2l.DATA_URL + &#39;timemachine.txt&#39;, &#39;090b5e7e70c295757f55df93cb0a180b9691891a&#39;) def read_time_machine(): #@save &#34;&#34;&#34;将时间机器数据集加载到文本行的列表中&#34;&#34;&#34; with open(d2l.download(&#39;time_machine&#39;), &#39;r&#39;) as f: lines = f.readlines() # &#39;[^A-Za-z]+&#39; 匹配 所有连续的非字母字符（空格、标点、数字等），并将其替换为空格 # .strip()删除字符串开头和结尾的空格 return [re.sub(&#39;[^A-Za-z]+&#39;, &#39; &#39;, line).strip().lower() for line in lines] # 把文本行拆分成词元（tokens），可以按单词或者按字符拆分 # lines为存储文本行的列表 def tokenize(lines, token=&#39;word&#39;): #@save &#34;&#34;&#34;将文本行拆分为单词或字符词元&#34;&#34;&#34; if token == &#39;word&#39;: # 注意line.split()本身是一个列表，这个分支情况的返回值形如：[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;], [&#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]] return [line.split() for line in lines] elif token == &#39;char&#39;: return [list(line) for line in lines] else: print(&#39;错误：未知词元类型：&#39; + token) def count_corpus(tokens): #@save &#34;&#34;&#34;统计词元的频率&#34;&#34;&#34; # 这里的tokens是1D列表或2D列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成一个列表（列表的嵌套表达式） # 外层：for line in tokens → 遍历 tokens 中的每一行（每一行本身是一个列表） # 内层：for token in line → 遍历这一行中的每个元素（词元） tokens = [token for line in tokens for token in line] # 返回的是一个字典，键是元素，值是出现次数。 return collections.Counter(tokens) # 词表类 class Vocab: &#34;&#34;&#34;文本词表&#34;&#34;&#34; def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] # reserved_tokens：保留的特殊词元列表 if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) # counter.items() 返回字典的 键值对列表，每个元素是 (key, value) 元组 # key=lambda x: x[1]表示按照value的值大小排序 self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # 生成一个列表，未知词元的索引为0 self.idx_to_token = [&#39;&lt;unk&gt;&#39;] + reserved_tokens # 生成一个字典，enumerate(self.idx_to_token) 会生成 (索引, 词元) 对 self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq &lt; min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) # 把一个词（或词列表）转换成它在词表中的索引 def __getitem__(self, tokens): # 如果tokens不是列表或元组 if not isinstance(tokens, (list, tuple)): # Python 字典 get 方法的标准用法，它接受 两个参数：dict.get(key, default) # 第一个参数 key：要查找的键，这里是 tokens（单个词）。 # 第二个参数 default：如果字典里没有这个键时返回的值，这里是 self.unk（未知词的索引，通常是 0）。 return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] # 把索引（或索引列表）转换回词 def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] # @property 是 Python 的一个 装饰器（decorator），用于把一个类的方法 变成属性（attribute） 来访问，而不需要加括号()调用 @property def unk(self): # 未知词元的索引为0 return 0 @property def token_freqs(self): return self._token_freqs # 返回时光机器数据集的词元索引列表和词表 def load_corpus_time_machine(max_tokens=-1): lines = read_time_machine() # 将每行文本拆成字符列表（因为 &#39;char&#39;），tokens 是一个二维列表 tokens = tokenize(lines, &#39;char&#39;) vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落， # 所以将所有文本行展平到一个列表中 # 当写 vocab[token] 时，Python 会自动调用 vocab.__getitem__(token)，返回对应的索引 # corpus是一个列表，按照原本文本的字符顺序，把各个字符在词表中对应的索引保存起来 corpus = [vocab[token] for line in tokens for token in line] if max_tokens &gt; 0: corpus = corpus[:max_tokens] return corpus, vocab # 使用随机抽样生成一个小批量子序列 # batch_size：每个小批量包含多少条子序列 # num_steps: 每个子序列的长度（即时间步数） def seq_data_iter_random(corpus, batch_size, num_steps): # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # num_subseqs表示可以切出多少个长度为 num_steps 的子序列 # 减去1，因为要给标签留一个位置 num_subseqs = (len(corpus) - 1) // num_steps # 构造所有子序列的起始索引，每隔 num_steps 取一个 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从pos位置开始的长度为num_steps的序列 return corpus[pos: pos + num_steps] # num_batches表示能完整分多少个 batch num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # 在这里，initial_indices包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) # 举例：假设corpus = list(range(30)),就是[0,1,2,...,29],batch_size = 2,num_steps = 5 # num_subseqs = (30-1)//5 = 5,所以一共可以切 5 段子序列，每段长 5： # [0,1,2,3,4] # [5,6,7,8,9] # [10,11,12,13,14] # [15,16,17,18,19] # [20,21,22,23,24] # 得到 initial_indices = [0, 5, 10, 15, 20] ,打乱后：[15, 0, 20, 10, 5] # num_batches = 5 // 2 = 2，所以总共能生成 2 个 batch。 # 循环展开： # i=0时：initial_indices_per_batch = [15, 0]，X = [data(15), data(0)]，Y = [data(16), data(1)] # 得到X = [[15,16,17,18,19],[0,1,2,3,4]]，Y = [[16,17,18,19,20],[1,2,3,4,5]] # i=2时：initial_indices_per_batch = [20, 10]，X = [data(20), data(10)]，Y = [data(21), data(11)] # 得到X = [[20,21,22,23,24],[10,11,12,13,14]]，Y = [[21,22,23,24,25],[11,12,13,14,15]] # 使用顺序分区生成一个小批量子序列 def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y class SeqDataLoader: #@save &#34;&#34;&#34;加载序列数据的迭代器&#34;&#34;&#34; def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = seq_data_iter_random else: self.data_iter_fn = seq_data_iter_sequential self.corpus, self.vocab = load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps # 当写 for X, Y in train_iter: 时，Python 会自动调用 train_iter.__iter__() def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps) def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000): &#34;&#34;&#34;返回时光机器数据集的迭代器和词表&#34;&#34;&#34; data_iter = SeqDataLoader( batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab 1 2 3 4 5 %matplotlib inline import math import torch from torch import nn from torch.nn import functional as F 1 2 batch_size, num_steps = 32, 35 train_iter, vocab = load_data_time_machine(batch_size, num_steps) 1 F.one_hot(torch.tensor([0, 2]), len(vocab)) 1 2 3 4 5 6 7 8 9 # 利用jupyter notebook测试上述代码 vocab[&#39;a&#39;] # 返回4 vocab.token_to_idx[&#39;a&#39;]==vocab[&#39;a&#39;] # 返回True vocab.idx_to_token[0] # 返回&#39;&lt;unk&gt;&#39; vocab.idx_to_token[2] # 返回&#39;e&#39; # 返回torch.Size([5, 2, 28]) X = torch.arange(10).reshape((2, 5)) F.one_hot(X.T, 28).shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def get_params(vocab_size, num_hiddens, device): # 词表长度为28，因为输入时需要将每一个token进行独热编码，则此处输入长度应该也为28 num_inputs = num_outputs = vocab_size def normal(shape): # randn 的意思就是 random normal，它会从标准正态分布中采样数据，乘0.01后服从标准差为0.01的高斯分布 return torch.randn(size=shape, device=device) * 0.01 # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # 隐藏层参数 W_xh = normal((num_inputs, num_hiddens)) W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # O_t = H_t·W_hq + b_q # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params 1 2 3 #返回隐藏层初始状态，此处为全零 def init_rnn_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # inputs形状为 (num_steps, batch_size, vocab_size) # state形状为 (batch_size, num_hiddens) def rnn(inputs, state, params): # inputs的形状：(时间步数量，批量大小，词表大小) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # X的形状：(batch_size, vocab_size) for X in inputs: # H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h ) # (batch_size, num_hiddens) = (batch_size, vocab_size)*(vocab_size, num_hiddens) + (batch_size, num_hiddens)*(num_hiddens, num_hiddens) + (num_hiddens,) # 其中b_h形状(num_hiddens,)会被解释成(1, num_hiddens) → 自动广播成 (batch_size, num_hiddens) # 在默认的一维 bias 情况下，PyTorch 总是在最前面加一个维度 → (1, num_hiddens),而不会被解释成(num_hiddens,1) H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) # O_t = H_t·W_hq + b_q # (batch_size, vocab_size) = (batch_size, num_hiddens)*(num_hiddens, vocab_size) + (vocab_size,) # # 其中b_q形状(vocab_size,)会被解释成(1, vocab_size) → 自动广播成 (batch_size, vocab_size) Y = torch.mm(H, W_hq) + b_q outputs.append(Y) # 沿着第一个维度将outputs数组进行拼接 # outputs本身为长度为num_steps的列表，outputs[i] 的形状就是 (batch_size, vocab_size) # 拼接后形状为(num_steps*batch_size, vocab_size) return torch.cat(outputs, dim=0), (H,) 1 2 3 4 5 6 7 8 9 10 11 12 13 class RNNModelScratch: &#34;&#34;&#34;从零开始实现的循环神经网络模型&#34;&#34;&#34; def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn # __call__ 函数在用类实例像函数一样调用时会自动执行 def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) 1 num_hiddens = 512 1 2 3 4 5 6 7 8 # net是类的实例对象 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) # 实例对象被像函数一样调用时自动触发call函数，net(...)等价于net.__call__(...) Y, new_state = net(X.to(d2l.try_gpu()), state) Y.shape, len(new_state), new_state[0].shape 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # prefix 是函数的一个输入参数，表示生成文本的前缀字符串 def predict_ch8(prefix, num_preds, net, vocab, device): &#34;&#34;&#34;在prefix后面生成新字符&#34;&#34;&#34; state = net.begin_state(batch_size=1, device=device) # 把 prefix 的第一个字符转换成对应的索引（通过词表 vocab）加入 outputs 列表 outputs = [vocab[prefix[0]]] # 定义一个匿名函数，每次调用返回当前 RNN 的输入，outputs[-1] 是上一个生成的字符的索引 get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) # 预热期，让 RNN 根据前缀 prefix “热身”，更新隐藏状态 for y in prefix[1:]: # _ 表示输出暂时不用，只更新隐藏状态，即state 会被更新，用于下一个时间步 _, state = net(get_input(), state) # 同时把前缀中对应字符的索引追加到 outputs列表中 outputs.append(vocab[y]) # 预测num_preds步 for _ in range(num_preds): y, state = net(get_input(), state) print(&#34;y的形状：&#34;,y.shape) print(&#34;y中得分最高的索引：&#34;,y.argmax(dim=1)) print(&#34;当前预测字符：&#34;,vocab.idx_to_token[int(y.argmax(dim=1))]) # 把预测出来的对应字符的索引追加到 outputs列表中 # y.argmax(dim=1) 取得分最高的索引 → 当前预测字符 outputs.append(int(y.argmax(dim=1).reshape(1))) print(&#34;outputs列表:&#34;,outputs) print(&#34;\\n&#34;) return &#39;&#39;.join([vocab.idx_to_token[i] for i in outputs]) 1 predict_ch8(&#39;time traveller &#39;, 10, net, vocab, d2l.try_gpu()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 梯度裁剪，防止梯度爆炸 def grad_clipping(net, theta): # 如果 net 是 PyTorch 模块，直接用 net.parameters() 获取可训练参数 if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params # 计算梯度范数（对所有参数的梯度求平方和，然后开平方） # 这一行的norm是一个标量，是params里所有参数（每一个参数是一个矩阵）的平方和在开方 norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) if norm &gt; theta: for param in params: # 修改过后的梯度向量的L2范数为theta param.grad[:] *= theta / norm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # 训练一个 epoch def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): state, timer = None, d2l.Timer() # 总损失 metric[0]，总词元数 metric[1] metric = d2l.Accumulator(2) # X形状为(batch_size, num_steps)，Y与X形状相同 for X, Y in train_iter: # 第一次迭代，或者使用随机采样时，每个小批量都重新初始化隐藏状态 if state is None or use_random_iter: state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state对于nn.GRU是个张量 state.detach_() else: # state对于nn.LSTM或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() # 先将Y的形状转置为(num_steps, batch_size),再按行展平成一维向量，形状为(batch_size * num_steps,) # 为什么要先转置？ 因为希望按照时间步顺序展平而不是批量顺序 # 展平后为（第1个时间步每个batch的第一个元素，然后第2个时间步每个batch的第二个元素，...） # 因为后续y_hat形状为(batch_size∗num_steps,vocab_size) # PyTorch 的 CrossEntropyLoss 要求： # input：形状 (N, C)，N 是样本数，C 是类别数。 # target：形状 (N,)，一维整数向量表示每个样本的类别。 y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1) updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了mean函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 训练整个RNN模型 def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;perplexity&#39;, legend=[&#39;train&#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(&#39;time traveller&#39;)) animator.add(epoch + 1, [ppl]) print(f&#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}&#39;) print(predict(&#39;time traveller&#39;)) print(predict(&#39;traveller&#39;)) 1 2 3 # 顺序分区（sequential） 生成训练小批量 num_epochs, lr = 500, 1 train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu()) 1 2 3 4 5 # 随机抽样（random）生成训练小批量 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True) "><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://example.com/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/index.png' />
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/tx_hu_1ac2efbf4b10a9c1.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">😍</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">王涑毓的个人博客</a></h1>
            <h2 class="site-description">这是我的个人博客网站</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://space.bilibili.com/287094222'
                        target="_blank"
                        title="Bilibili"
                        rel="me"
                    >
                        
                        
                            <svg  xmlns="http://www.w3.org/2000/svg"  width="24"  height="24"  viewBox="0 0 24 24"  fill="none"  stroke="currentColor"  stroke-width="2"  stroke-linecap="round"  stroke-linejoin="round"  class="icon icon-tabler icons-tabler-outline icon-tabler-brand-bilibili"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 10a4 4 0 0 1 4 -4h10a4 4 0 0 1 4 4v6a4 4 0 0 1 -4 4h-10a4 4 0 0 1 -4 -4v-6z" /><path d="M8 3l2 3" /><path d="M16 3l-2 3" /><path d="M9 13v-2" /><path d="M15 11v2" /></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/chichusy'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E9%93%BE%E6%8E%A5/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>链接</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/index_hu_f49854331d99157b.png"
                        srcset="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/index_hu_f49854331d99157b.png 800w, /p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/index_hu_a4333dc57850a2b8.png 1600w"
                        width="800" 
                        height="450" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-8.5. 循环神经网络的从零开始实现" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E5%AD%A6%E4%B9%A0/" >
                学习
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-8.5.-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0/">动手学深度学习-8.5. 循环神经网络的从零开始实现</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-08-19</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 11 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h1 id="动手学深度学习-85-循环神经网络的从零开始实现">动手学深度学习-8.5. 循环神经网络的从零开始实现
</h1><hr>
<h2 id="代码">代码
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">collections</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">re</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># d2l.DATA_HUB是一个字典，用于存储数据集的下载地址和文件校验信息（哈希值）</span>
</span></span><span class="line"><span class="cl"><span class="c1"># key：数据集名称 &#39;time_machine&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># value：一个 tuple (url, sha1)</span>
</span></span><span class="line"><span class="cl"><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_HUB</span><span class="p">[</span><span class="s1">&#39;time_machine&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_URL</span> <span class="o">+</span> <span class="s1">&#39;timemachine.txt&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="s1">&#39;090b5e7e70c295757f55df93cb0a180b9691891a&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">read_time_machine</span><span class="p">():</span>  <span class="c1">#@save</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;将时间机器数据集加载到文本行的列表中&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;time_machine&#39;</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># &#39;[^A-Za-z]+&#39; 匹配 所有连续的非字母字符（空格、标点、数字等），并将其替换为空格</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># .strip()删除字符串开头和结尾的空格</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[^A-Za-z]+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 把文本行拆分成词元（tokens），可以按单词或者按字符拆分</span>
</span></span><span class="line"><span class="cl"><span class="c1"># lines为存储文本行的列表</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">):</span>  <span class="c1">#@save</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;将文本行拆分为单词或字符词元&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="s1">&#39;word&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意line.split()本身是一个列表，这个分支情况的返回值形如：[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;], [&#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">token</span> <span class="o">==</span> <span class="s1">&#39;char&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;错误：未知词元类型：&#39;</span> <span class="o">+</span> <span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">count_corpus</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>  <span class="c1">#@save</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;统计词元的频率&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这里的tokens是1D列表或2D列表</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将词元列表展平成一个列表（列表的嵌套表达式）</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 外层：for line in tokens → 遍历 tokens 中的每一行（每一行本身是一个列表）</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 内层：for token in line → 遍历这一行中的每个元素（词元）</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 返回的是一个字典，键是元素，值是出现次数。</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 词表类</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Vocab</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;文本词表&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reserved_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reserved_tokens：保留的特殊词元列表</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">reserved_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">reserved_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 按出现频率排序</span>
</span></span><span class="line"><span class="cl">        <span class="n">counter</span> <span class="o">=</span> <span class="n">count_corpus</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># counter.items() 返回字典的 键值对列表，每个元素是 (key, value) 元组</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># key=lambda x: x[1]表示按照value的值大小排序</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 生成一个列表，未知词元的索引为0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">reserved_tokens</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 生成一个字典，enumerate(self.idx_to_token) 会生成 (索引, 词元) 对</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span>
</span></span><span class="line"><span class="cl">                             <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)}</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">freq</span> <span class="o">&lt;</span> <span class="n">min_freq</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 把一个词（或词列表）转换成它在词表中的索引</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果tokens不是列表或元组</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Python 字典 get 方法的标准用法，它接受 两个参数：dict.get(key, default)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一个参数 key：要查找的键，这里是 tokens（单个词）。</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第二个参数 default：如果字典里没有这个键时返回的值，这里是 self.unk（未知词的索引，通常是 0）。</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 把索引（或索引列表）转换回词</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">to_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># @property 是 Python 的一个 装饰器（decorator），用于把一个类的方法 变成属性（attribute） 来访问，而不需要加括号()调用</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">unk</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># 未知词元的索引为0</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">token_freqs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_token_freqs</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1"># 返回时光机器数据集的词元索引列表和词表</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_corpus_time_machine</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span> 
</span></span><span class="line"><span class="cl">    <span class="n">lines</span> <span class="o">=</span> <span class="n">read_time_machine</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 将每行文本拆成字符列表（因为 &#39;char&#39;），tokens 是一个二维列表</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="s1">&#39;char&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 所以将所有文本行展平到一个列表中</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 当写 vocab[token] 时，Python 会自动调用 vocab.__getitem__(token)，返回对应的索引</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># corpus是一个列表，按照原本文本的字符顺序，把各个字符在词表中对应的索引保存起来</span>
</span></span><span class="line"><span class="cl">    <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">max_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[:</span><span class="n">max_tokens</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">vocab</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用随机抽样生成一个小批量子序列</span>
</span></span><span class="line"><span class="cl"><span class="c1"># batch_size：每个小批量包含多少条子序列</span>
</span></span><span class="line"><span class="cl"><span class="c1"># num_steps: 每个子序列的长度（即时间步数）</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">seq_data_iter_random</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span>
</span></span><span class="line"><span class="cl">    <span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># num_subseqs表示可以切出多少个长度为 num_steps 的子序列</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 减去1，因为要给标签留一个位置</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_subseqs</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_steps</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 构造所有子序列的起始索引，每隔 num_steps 取一个</span>
</span></span><span class="line"><span class="cl">    <span class="n">initial_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_subseqs</span> <span class="o">*</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 在随机抽样的迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span>
</span></span><span class="line"><span class="cl">    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">initial_indices</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="n">pos</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 返回从pos位置开始的长度为num_steps的序列</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">corpus</span><span class="p">[</span><span class="n">pos</span><span class="p">:</span> <span class="n">pos</span> <span class="o">+</span> <span class="n">num_steps</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># num_batches表示能完整分多少个 batch</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_batches</span> <span class="o">=</span> <span class="n">num_subseqs</span> <span class="o">//</span> <span class="n">batch_size</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_batches</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在这里，initial_indices包含子序列的随机起始索引</span>
</span></span><span class="line"><span class="cl">        <span class="n">initial_indices_per_batch</span> <span class="o">=</span> <span class="n">initial_indices</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">initial_indices_per_batch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">initial_indices_per_batch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 举例：假设corpus = list(range(30)),就是[0,1,2,...,29],batch_size = 2,num_steps = 5</span>
</span></span><span class="line"><span class="cl"><span class="c1"># num_subseqs = (30-1)//5 = 5,所以一共可以切 5 段子序列，每段长 5：</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [0,1,2,3,4]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [5,6,7,8,9]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [10,11,12,13,14]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [15,16,17,18,19]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [20,21,22,23,24]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 得到 initial_indices = [0, 5, 10, 15, 20] ,打乱后：[15, 0, 20, 10, 5]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># num_batches = 5 // 2 = 2，所以总共能生成 2 个 batch。</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 循环展开：</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i=0时：initial_indices_per_batch = [15, 0]，X = [data(15), data(0)]，Y = [data(16), data(1)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 得到X = [[15,16,17,18,19],[0,1,2,3,4]]，Y = [[16,17,18,19,20],[1,2,3,4,5]]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i=2时：initial_indices_per_batch = [20, 10]，X = [data(20), data(10)]，Y = [data(21), data(11)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 得到X = [[20,21,22,23,24],[10,11,12,13,14]]，Y = [[21,22,23,24,25],[11,12,13,14,15]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用顺序分区生成一个小批量子序列</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">seq_data_iter_sequential</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>  <span class="c1">#@save</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 从随机偏移量开始划分序列</span>
</span></span><span class="line"><span class="cl">    <span class="n">offset</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_tokens</span> <span class="o">=</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> <span class="o">-</span> <span class="n">offset</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
</span></span><span class="line"><span class="cl">    <span class="n">Xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">num_tokens</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">num_tokens</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">Xs</span><span class="p">,</span> <span class="n">Ys</span> <span class="o">=</span> <span class="n">Xs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">Ys</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_batches</span> <span class="o">=</span> <span class="n">Xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_steps</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">*</span> <span class="n">num_batches</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="n">Xs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_steps</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">Ys</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_steps</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">yield</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SeqDataLoader</span><span class="p">:</span>  <span class="c1">#@save</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;加载序列数据的迭代器&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">use_random_iter</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">use_random_iter</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">data_iter_fn</span> <span class="o">=</span> <span class="n">seq_data_iter_random</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">data_iter_fn</span> <span class="o">=</span> <span class="n">seq_data_iter_sequential</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">load_corpus_time_machine</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 当写 for X, Y in train_iter: 时，Python 会自动调用 train_iter.__iter__()</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_iter_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_steps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_data_time_machine</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">use_random_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;返回时光机器数据集的迭代器和词表&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">data_iter</span> <span class="o">=</span> <span class="n">SeqDataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">use_random_iter</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">data_iter</span><span class="o">.</span><span class="n">vocab</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">35</span>
</span></span><span class="line"><span class="cl"><span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">load_data_time_machine</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 利用jupyter notebook测试上述代码</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span> <span class="c1"># 返回4</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span><span class="o">.</span><span class="n">token_to_idx</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span> <span class="c1"># 返回True</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 返回&#39;&lt;unk&gt;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">vocab</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># 返回&#39;e&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 返回torch.Size([5, 2, 28])</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 词表长度为28，因为输入时需要将每一个token进行独热编码，则此处输入长度应该也为28 </span>
</span></span><span class="line"><span class="cl">    <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># randn 的意思就是 random normal，它会从标准正态分布中采样数据，乘0.01后服从标准差为0.01的高斯分布</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h )</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 隐藏层参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">W_xh</span> <span class="o">=</span> <span class="n">normal</span><span class="p">((</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">W_hh</span> <span class="o">=</span> <span class="n">normal</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># O_t = H_t·W_hq + b_q</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 输出层参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">W_hq</span> <span class="o">=</span> <span class="n">normal</span><span class="p">((</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 附加梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">W_xh</span><span class="p">,</span> <span class="n">W_hh</span><span class="p">,</span> <span class="n">b_h</span><span class="p">,</span> <span class="n">W_hq</span><span class="p">,</span> <span class="n">b_q</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">params</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#返回隐藏层初始状态，此处为全零</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_rnn_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># inputs形状为 (num_steps, batch_size, vocab_size)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># state形状为 (batch_size, num_hiddens)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rnn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># inputs的形状：(时间步数量，批量大小，词表大小)</span>
</span></span><span class="line"><span class="cl">    <span class="n">W_xh</span><span class="p">,</span> <span class="n">W_hh</span><span class="p">,</span> <span class="n">b_h</span><span class="p">,</span> <span class="n">W_hq</span><span class="p">,</span> <span class="n">b_q</span> <span class="o">=</span> <span class="n">params</span>
</span></span><span class="line"><span class="cl">    <span class="n">H</span><span class="p">,</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># X的形状：(batch_size, vocab_size)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># H_t = tanh( X_t·W_xh + H_t-1·W_hh + b_h )</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (batch_size, num_hiddens) = (batch_size, vocab_size)*(vocab_size, num_hiddens) + (batch_size, num_hiddens)*(num_hiddens, num_hiddens) + (num_hiddens,)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 其中b_h形状(num_hiddens,)会被解释成(1, num_hiddens) → 自动广播成 (batch_size, num_hiddens)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在默认的一维 bias 情况下，PyTorch 总是在最前面加一个维度 → (1, num_hiddens),而不会被解释成(num_hiddens,1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W_xh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                       <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W_hh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                       <span class="o">+</span> <span class="n">b_h</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># O_t = H_t·W_hq + b_q</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (batch_size, vocab_size) = (batch_size, num_hiddens)*(num_hiddens, vocab_size) + (vocab_size,)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># # 其中b_q形状(vocab_size,)会被解释成(1, vocab_size) → 自动广播成 (batch_size, vocab_size)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W_hq</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_q</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 沿着第一个维度将outputs数组进行拼接</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># outputs本身为长度为num_steps的列表，outputs[i] 的形状就是 (batch_size, vocab_size)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 拼接后形状为(num_steps*batch_size, vocab_size)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">H</span><span class="p">,)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RNNModelScratch</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;从零开始实现的循环神经网络模型&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">get_params</span><span class="p">,</span> <span class="n">init_state</span><span class="p">,</span> <span class="n">forward_fn</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hiddens</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_fn</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span> <span class="n">forward_fn</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># __call__ 函数在用类实例像函数一样调用时会自动执行</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_fn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">begin_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hiddens</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">num_hiddens</span> <span class="o">=</span> <span class="mi">512</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># net是类的实例对象</span>
</span></span><span class="line"><span class="cl"><span class="n">net</span> <span class="o">=</span> <span class="n">RNNModelScratch</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">(),</span> <span class="n">get_params</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                      <span class="n">init_rnn_state</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 实例对象被像函数一样调用时自动触发call函数，net(...)等价于net.__call__(...)</span>
</span></span><span class="line"><span class="cl"><span class="n">Y</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">()),</span> <span class="n">state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_state</span><span class="p">),</span> <span class="n">new_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># prefix 是函数的一个输入参数，表示生成文本的前缀字符串</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">predict_ch8</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">num_preds</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;在prefix后面生成新字符&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 把 prefix 的第一个字符转换成对应的索引（通过词表 vocab）加入 outputs 列表</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">prefix</span><span class="p">[</span><span class="mi">0</span><span class="p">]]]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 定义一个匿名函数，每次调用返回当前 RNN 的输入，outputs[-1] 是上一个生成的字符的索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">get_input</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 预热期，让 RNN 根据前缀 prefix “热身”，更新隐藏状态</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># _ 表示输出暂时不用，只更新隐藏状态，即state 会被更新，用于下一个时间步</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">get_input</span><span class="p">(),</span> <span class="n">state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 同时把前缀中对应字符的索引追加到 outputs列表中</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="c1"># 预测num_preds步</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_preds</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">get_input</span><span class="p">(),</span> <span class="n">state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;y的形状：&#34;</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;y中得分最高的索引：&#34;</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;当前预测字符：&#34;</span><span class="p">,</span><span class="n">vocab</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 把预测出来的对应字符的索引追加到 outputs列表中</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># y.argmax(dim=1) 取得分最高的索引 → 当前预测字符</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;outputs列表:&#34;</span><span class="p">,</span><span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">vocab</span><span class="o">.</span><span class="n">idx_to_token</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">predict_ch8</span><span class="p">(</span><span class="s1">&#39;time traveller &#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div align=center>
    <img src="result1.png" alt="result1" style="zoom: 80%;" />
<div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 梯度裁剪，防止梯度爆炸</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grad_clipping</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果 net 是 PyTorch 模块，直接用 net.parameters() 获取可训练参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">params</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算梯度范数（对所有参数的梯度求平方和，然后开平方）</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这一行的norm是一个标量，是params里所有参数（每一个参数是一个矩阵）的平方和在开方</span>
</span></span><span class="line"><span class="cl">    <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">norm</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 修改过后的梯度向量的L2范数为theta</span>
</span></span><span class="line"><span class="cl">            <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">[:]</span> <span class="o">*=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">norm</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 训练一个 epoch</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_epoch_ch8</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updater</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">use_random_iter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span><span class="p">,</span> <span class="n">timer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 总损失 metric[0]，总词元数 metric[1]</span>
</span></span><span class="line"><span class="cl">    <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># X形状为(batch_size, num_steps)，Y与X形状相同</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">train_iter</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第一次迭代，或者使用随机采样时，每个小批量都重新初始化隐藏状态</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">use_random_iter</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">begin_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># state对于nn.GRU是个张量</span>
</span></span><span class="line"><span class="cl">                <span class="n">state</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">s</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 先将Y的形状转置为(num_steps, batch_size),再按行展平成一维向量，形状为(batch_size * num_steps,)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为什么要先转置？ 因为希望按照时间步顺序展平而不是批量顺序</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 展平后为（第1个时间步每个batch的第一个元素，然后第2个时间步每个batch的第二个元素，...）</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 因为后续y_hat形状为(batch_size∗num_steps,vocab_size)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># PyTorch 的 CrossEntropyLoss 要求：</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># input：形状 (N, C)，N 是样本数，C 是类别数。</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># target：形状 (N,)，一维整数向量表示每个样本的类别。</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_hat</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updater</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">updater</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">grad_clipping</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">updater</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">grad_clipping</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 因为已经调用了mean函数</span>
</span></span><span class="line"><span class="cl">            <span class="n">updater</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 训练整个RNN模型</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_ch8</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">use_random_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;perplexity&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">updater</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">updater</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">d2l</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predict</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">prefix</span><span class="p">:</span> <span class="n">predict_ch8</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 训练和预测</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">ppl</span><span class="p">,</span> <span class="n">speed</span> <span class="o">=</span> <span class="n">train_epoch_ch8</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">updater</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">use_random_iter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;time traveller&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">ppl</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;困惑度 </span><span class="si">{</span><span class="n">ppl</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">speed</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> 词元/秒 </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;time traveller&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;traveller&#39;</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 顺序分区（sequential） 生成训练小批量</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">train_ch8</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div align=center>
    <img src="result2.png" alt="result2" style="zoom: 80%;" />
<div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 随机抽样（random）生成训练小批量</span>
</span></span><span class="line"><span class="cl"><span class="n">net</span> <span class="o">=</span> <span class="n">RNNModelScratch</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">num_hiddens</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">(),</span> <span class="n">get_params</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                      <span class="n">init_rnn_state</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">train_ch8</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">          <span class="n">use_random_iter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div align=center>
    <img src="result3.png" alt="result3" style="zoom: 80%;" />
<div>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/">工程实践</a>
        
            <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">动手学深度学习</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Ending of the article</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6.6.-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6.6.-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/index.e6e72e0c91787d9d394d8c219684b716_hu_ff4c539dcd7c97de.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-6.6. 卷积神经网络（LeNet）"
                        
                        data-hash="md5-5ucuDJF4fZ05TYwhloS3Fg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动手学深度学习-6.6. 卷积神经网络（LeNet）</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.10.-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.10.-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/index.3b94ae80619b72c051c6977aab7a1593_hu_ed3d98481c1d8087.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-4.10. 实战Kaggle比赛：预测房价"
                        
                        data-hash="md5-O5SugGGbcsBRxpd6q3oVkw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动手学深度学习-4.10. 实战Kaggle比赛：预测房价</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.6.-%E6%9A%82%E9%80%80%E6%B3%95dropout/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.6.-%E6%9A%82%E9%80%80%E6%B3%95dropout/index.34c033d3ae916728e0097f5da5f2485e_hu_ed75a432618aa1fc.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-4.6. 暂退法（Dropout）"
                        
                        data-hash="md5-NMAz066RZyjgCX9dpfJIXg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动手学深度学习-4.6. 暂退法（Dropout）</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.5.-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.5.-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/index.4d851afd09f5d7c7ef7d4420fd9a2d86_hu_eda410826cd73b44.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-4.5. 权重衰减"
                        
                        data-hash="md5-TYUa/Qn118fvfUQg/Zothg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动手学深度学习-4.5. 权重衰减</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.3.-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4.3.-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/index.a0c2c9a5bf1da022bfdcbbf6453fa436_hu_c9f179a39bbb64ad.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动手学深度学习-4.3. 多层感知机的简洁实现"
                        
                        data-hash="md5-oMLJpb8doCK/3Lv2RT&#43;kNg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动手学深度学习-4.3. 多层感知机的简洁实现</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2025 Wang Suyu
    </section>
    
    <section class="powerby">
        
            此网站仅用来记录个人学习生活 <br/>
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.27.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>
<script
    src="https://cdn.jsdelivr.net/gh/zhixuan2333/gh-blog@v0.1.0/js/ribbon.min.js"
    integrity="sha384-UEK8ZiP3VgFNP8KnKMKDmd4pAUAOJ59Y2Jo3ED2Z5qKQf6HLHovMxq7Beb9CLPUe"
    crossorigin="anonymous"
    size="300"
    alpha="0.2"
    zindex="-1"
    defer
></script>

    </body>
</html>
